6
1
0
2

 

n
u
J
 

6
1

 
 
]

G
L
.
s
c
[
 
 

2
v
3
8
7
1
0

.

2
0
6
1
:
v
i
X
r
a

Asynchronous Methods for Deep Reinforcement Learning

Volodymyr Mnih1
Adrià Puigdomènech Badia1
Mehdi Mirza1,2
Alex Graves1
Tim Harley1
Timothy P. Lillicrap1
David Silver1
Koray Kavukcuoglu 1
1 Google DeepMind
2 Montreal Institute for Learning Algorithms (MILA), University of Montreal

VMNIH@GOOGLE.COM
ADRIAP@GOOGLE.COM
MIRZAMOM@IRO.UMONTREAL.CA
GRAVESA@GOOGLE.COM
THARLEY@GOOGLE.COM
COUNTZERO@GOOGLE.COM
DAVIDSILVER@GOOGLE.COM
KORAYK@GOOGLE.COM

Abstract
conceptually

a

simple

propose

and
We
lightweight
framework for deep reinforce-
ment learning that uses asynchronous gradient
descent for optimization of deep neural network
controllers. We present asynchronous variants of
four standard reinforcement learning algorithms
and show that parallel actor-learners have a
stabilizing effect on training allowing all four
methods to successfully train neural network
controllers. The best performing method, an
asynchronous variant of actor-critic, surpasses
the current state-of-the-art on the Atari domain
while training for half the time on a single
multi-core CPU instead of a GPU. Furthermore,
we show that asynchronous actor-critic succeeds
on a wide variety of continuous motor control
problems as well as on a new task of navigating
random 3D mazes using a visual input.

1. Introduction
Deep neural networks provide rich representations that can
enable reinforcement learning (RL) algorithms to perform
effectively. However, it was previously thought that the
combination of simple online RL algorithms with deep
neural networks was fundamentally unstable. Instead, a va-
riety of solutions have been proposed to stabilize the algo-
rithm (Riedmiller, 2005; Mnih et al., 2013; 2015; Van Has-
selt et al., 2015; Schulman et al., 2015a). These approaches
share a common idea: the sequence of observed data en-
countered by an online RL agent is non-stationary, and on-

Proceedings of the 33 rd International Conference on Machine
Learning, New York, NY, USA, 2016. JMLR: W&CP volume
48. Copyright 2016 by the author(s).

line RL updates are strongly correlated. By storing the
agent’s data in an experience replay memory, the data can
be batched (Riedmiller, 2005; Schulman et al., 2015a) or
randomly sampled (Mnih et al., 2013; 2015; Van Hasselt
et al., 2015) from different time-steps. Aggregating over
memory in this way reduces non-stationarity and decorre-
lates updates, but at the same time limits the methods to
off-policy reinforcement learning algorithms.
Deep RL algorithms based on experience replay have
achieved unprecedented success in challenging domains
such as Atari 2600. However, experience replay has several
drawbacks: it uses more memory and computation per real
interaction; and it requires off-policy learning algorithms
that can update from data generated by an older policy.
In this paper we provide a very different paradigm for deep
reinforcement learning. Instead of experience replay, we
asynchronously execute multiple agents in parallel, on mul-
tiple instances of the environment. This parallelism also
decorrelates the agents’ data into a more stationary process,
since at any given time-step the parallel agents will be ex-
periencing a variety of different states. This simple idea
enables a much larger spectrum of fundamental on-policy
RL algorithms, such as Sarsa, n-step methods, and actor-
critic methods, as well as off-policy RL algorithms such
as Q-learning, to be applied robustly and effectively using
deep neural networks.
Our parallel reinforcement learning paradigm also offers
practical beneﬁts. Whereas previous approaches to deep re-
inforcement learning rely heavily on specialized hardware
such as GPUs (Mnih et al., 2015; Van Hasselt et al., 2015;
Schaul et al., 2015) or massively distributed architectures
(Nair et al., 2015), our experiments run on a single machine
with a standard multi-core CPU. When applied to a vari-
ety of Atari 2600 domains, on many games asynchronous
reinforcement learning achieves better results, in far less

Asynchronous Methods for Deep Reinforcement Learning

time than previous GPU-based algorithms, using far less
resource than massively distributed approaches. The best
of the proposed methods, asynchronous advantage actor-
critic (A3C), also mastered a variety of continuous motor
control tasks as well as learned general strategies for ex-
ploring 3D mazes purely from visual inputs. We believe
that the success of A3C on both 2D and 3D games, discrete
and continuous action spaces, as well as its ability to train
feedforward and recurrent agents makes it the most general
and successful reinforcement learning agent to date.

2. Related Work
The General Reinforcement Learning Architecture (Gorila)
of (Nair et al., 2015) performs asynchronous training of re-
inforcement learning agents in a distributed setting. In Go-
rila, each process contains an actor that acts in its own copy
of the environment, a separate replay memory, and a learner
that samples data from the replay memory and computes
gradients of the DQN loss (Mnih et al., 2015) with respect
to the policy parameters. The gradients are asynchronously
sent to a central parameter server which updates a central
copy of the model. The updated policy parameters are sent
to the actor-learners at ﬁxed intervals. By using 100 sep-
arate actor-learner processes and 30 parameter server in-
stances, a total of 130 machines, Gorila was able to signif-
icantly outperform DQN over 49 Atari games. On many
games Gorila reached the score achieved by DQN over 20
times faster than DQN. We also note that a similar way of
parallelizing DQN was proposed by (Chavez et al., 2015).
In earlier work, (Li & Schuurmans, 2011) applied the
Map Reduce framework to parallelizing batch reinforce-
ment learning methods with linear function approximation.
Parallelism was used to speed up large matrix operations
but not to parallelize the collection of experience or sta-
bilize learning. (Grounds & Kudenko, 2008) proposed a
parallel version of the Sarsa algorithm that uses multiple
separate actor-learners to accelerate training. Each actor-
learner learns separately and periodically sends updates to
weights that have changed signiﬁcantly to the other learn-
ers using peer-to-peer communication.
(Tsitsiklis, 1994) studied convergence properties of Q-
learning in the asynchronous optimization setting. These
results show that Q-learning is still guaranteed to converge
when some of the information is outdated as long as out-
dated information is always eventually discarded and sev-
eral other technical assumptions are satisﬁed. Even earlier,
(Bertsekas, 1982) studied the related problem of distributed
dynamic programming.
Another related area of work is in evolutionary meth-
ods, which are often straightforward to parallelize by dis-
tributing ﬁtness evaluations over multiple machines or
threads (Tomassini, 1999). Such parallel evolutionary ap-

proaches have recently been applied to some visual rein-
forcement learning tasks. In one example, (Koutník et al.,
2014) evolved convolutional neural network controllers for
the TORCS driving simulator by performing ﬁtness evalu-
ations on 8 CPU cores in parallel.

Rt = (cid:80)∞

3. Reinforcement Learning Background
We consider the standard reinforcement learning setting
where an agent interacts with an environment E over a
number of discrete time steps. At each time step t, the
agent receives a state st and selects an action at from some
set of possible actions A according to its policy π, where
π is a mapping from states st to actions at. In return, the
agent receives the next state st+1 and receives a scalar re-
ward rt. The process continues until the agent reaches a
terminal state after which the process restarts. The return
k=0 γkrt+k is the total accumulated return from
time step t with discount factor γ ∈ (0, 1]. The goal of the
agent is to maximize the expected return from each state st.
The action value Qπ(s, a) = E [Rt|st = s, a] is the ex-
pected return for selecting action a in state s and follow-
ing policy π. The optimal value function Q∗(s, a) =
maxπ Qπ(s, a) gives the maximum action value for state
s and action a achievable by any policy. Similarly, the
value of state s under policy π is deﬁned as V π(s) =
E [Rt|st = s] and is simply the expected return for follow-
ing policy π from state s.
In value-based model-free reinforcement learning methods,
the action value function is represented using a function ap-
proximator, such as a neural network. Let Q(s, a; θ) be an
approximate action-value function with parameters θ. The
updates to θ can be derived from a variety of reinforcement
learning algorithms. One example of such an algorithm is
Q-learning, which aims to directly approximate the optimal
action value function: Q∗(s, a) ≈ Q(s, a; θ). In one-step
Q-learning, the parameters θ of the action value function
Q(s, a; θ) are learned by iteratively minimizing a sequence
(cid:17)2
of loss functions, where the ith loss function deﬁned as
a(cid:48) Q(s(cid:48), a(cid:48); θi−1) − Q(s, a; θi)

Li(θi) = E(cid:16)

r + γ max

where s(cid:48) is the state encountered after state s.
We refer to the above method as one-step Q-learning be-
cause it updates the action value Q(s, a) toward the one-
step return r + γ maxa(cid:48) Q(s(cid:48), a(cid:48); θ). One drawback of us-
ing one-step methods is that obtaining a reward r only di-
rectly affects the value of the state action pair s, a that led
to the reward. The values of other state action pairs are
affected only indirectly through the updated value Q(s, a).
This can make the learning process slow since many up-
dates are required the propagate a reward to the relevant
preceding states and actions.

Asynchronous Methods for Deep Reinforcement Learning

One way of propagating rewards faster is by using n-
step returns (Watkins, 1989; Peng & Williams, 1996).
In n-step Q-learning, Q(s, a) is updated toward the n-
step return deﬁned as rt + γrt+1 + ··· + γn−1rt+n−1 +
maxa γnQ(st+n, a). This results in a single reward r di-
rectly affecting the values of n preceding state action pairs.
This makes the process of propagating rewards to relevant
state-action pairs potentially much more efﬁcient.
In contrast to value-based methods, policy-based model-
free methods directly parameterize the policy π(a|s; θ) and
update the parameters θ by performing, typically approx-
imate, gradient ascent on E[Rt]. One example of such
a method is the REINFORCE family of algorithms due
to Williams (1992). Standard REINFORCE updates the
policy parameters θ in the direction ∇θ log π(at|st; θ)Rt,
which is an unbiased estimate of ∇θE[Rt]. It is possible to
reduce the variance of this estimate while keeping it unbi-
ased by subtracting a learned function of the state bt(st),
known as a baseline (Williams, 1992), from the return. The
resulting gradient is ∇θ log π(at|st; θ) (Rt − bt(st)).
A learned estimate of the value function is commonly used
as the baseline bt(st) ≈ V π(st) leading to a much lower
variance estimate of the policy gradient. When an approx-
imate value function is used as the baseline, the quantity
Rt − bt used to scale the policy gradient can be seen as
an estimate of the advantage of action at in state st, or
A(at, st) = Q(at, st)−V (st), because Rt is an estimate of
Qπ(at, st) and bt is an estimate of V π(st). This approach
can be viewed as an actor-critic architecture where the pol-
icy π is the actor and the baseline bt is the critic (Sutton &
Barto, 1998; Degris et al., 2012).

4. Asynchronous RL Framework
We now present multi-threaded asynchronous variants of
one-step Sarsa, one-step Q-learning, n-step Q-learning, and
advantage actor-critic. The aim in designing these methods
was to ﬁnd RL algorithms that can train deep neural net-
work policies reliably and without large resource require-
ments. While the underlying RL methods are quite dif-
ferent, with actor-critic being an on-policy policy search
method and Q-learning being an off-policy value-based
method, we use two main ideas to make all four algorithms
practical given our design goal.
First, we use asynchronous actor-learners, similarly to the
Gorila framework (Nair et al., 2015), but instead of using
separate machines and a parameter server, we use multi-
ple CPU threads on a single machine. Keeping the learn-
ers on a single machine removes the communication costs
of sending gradients and parameters and enables us to use
Hogwild! (Recht et al., 2011) style updates for training.
Second, we make the observation that multiple actors-

Algorithm 1 Asynchronous one-step Q-learning - pseu-
docode for each actor-learner thread.

// Assume global shared θ, θ−, and counter T = 0.
Initialize thread step counter t ← 0
Initialize target network weights θ− ← θ
Initialize network gradients dθ ← 0
Get initial state s
repeat

(cid:26) r

Take action a with -greedy policy based on Q(s, a; θ)
Receive new state s(cid:48) and reward r
for terminal s(cid:48)
r + γ maxa(cid:48) Q(s(cid:48), a(cid:48); θ−)
for non-terminal s(cid:48)
y =
Accumulate gradients wrt θ: dθ ← dθ + ∂(y−Q(s,a;θ))2
s = s(cid:48)
T ← T + 1 and t ← t + 1
if T mod Itarget == 0 then

∂θ

Update the target network θ− ← θ

end if
if t mod IAsyncU pdate == 0 or s is terminal then

Perform asynchronous update of θ using dθ.
Clear gradients dθ ← 0.

end if

until T > Tmax

learners running in parallel are likely to be exploring dif-
ferent parts of the environment. Moreover, one can explic-
itly use different exploration policies in each actor-learner
to maximize this diversity. By running different explo-
ration policies in different threads, the overall changes be-
ing made to the parameters by multiple actor-learners ap-
plying online updates in parallel are likely to be less corre-
lated in time than a single agent applying online updates.
Hence, we do not use a replay memory and rely on parallel
actors employing different exploration policies to perform
the stabilizing role undertaken by experience replay in the
DQN training algorithm.
In addition to stabilizing learning, using multiple parallel
actor-learners has multiple practical beneﬁts. First, we ob-
tain a reduction in training time that is roughly linear in
the number of parallel actor-learners. Second, since we no
longer rely on experience replay for stabilizing learning we
are able to use on-policy reinforcement learning methods
such as Sarsa and actor-critic to train neural networks in a
stable way. We now describe our variants of one-step Q-
learning, one-step Sarsa, n-step Q-learning and advantage
actor-critic.
Asynchronous one-step Q-learning: Pseudocode for our
variant of Q-learning, which we call Asynchronous one-
step Q-learning, is shown in Algorithm 1. Each thread in-
teracts with its own copy of the environment and at each
step computes a gradient of the Q-learning loss. We use
a shared and slowly changing target network in comput-
ing the Q-learning loss, as was proposed in the DQN train-
ing method. We also accumulate gradients over multiple
timesteps before they are applied, which is similar to us-

Asynchronous Methods for Deep Reinforcement Learning

ing minibatches. This reduces the chances of multiple ac-
tor learners overwriting each other’s updates. Accumulat-
ing updates over several steps also provides some ability to
trade off computational efﬁciency for data efﬁciency.
Finally, we found that giving each thread a different explo-
ration policy helps improve robustness. Adding diversity
to exploration in this manner also generally improves per-
formance through better exploration. While there are many
possible ways of making the exploration policies differ we
experiment with using -greedy exploration with  periodi-
cally sampled from some distribution by each thread.
Asynchronous one-step Sarsa: The asynchronous one-
step Sarsa algorithm is the same as asynchronous one-step
Q-learning as given in Algorithm 1 except that it uses a dif-
ferent target value for Q(s, a). The target value used by
one-step Sarsa is r + γQ(s(cid:48), a(cid:48); θ−) where a(cid:48) is the action
taken in state s(cid:48) (Rummery & Niranjan, 1994; Sutton &
Barto, 1998). We again use a target network and updates
accumulated over multiple timesteps to stabilize learning.
Asynchronous n-step Q-learning: Pseudocode for our
variant of multi-step Q-learning is shown in Supplementary
Algorithm S2. The algorithm is somewhat unusual because
it operates in the forward view by explicitly computing n-
step returns, as opposed to the more common backward
view used by techniques like eligibility traces (Sutton &
Barto, 1998). We found that using the forward view is eas-
ier when training neural networks with momentum-based
methods and backpropagation through time.
In order to
compute a single update, the algorithm ﬁrst selects actions
using its exploration policy for up to tmax steps or until a
terminal state is reached. This process results in the agent
receiving up to tmax rewards from the environment since
its last update. The algorithm then computes gradients for
n-step Q-learning updates for each of the state-action pairs
encountered since the last update. Each n-step update uses
the longest possible n-step return resulting in a one-step
update for the last state, a two-step update for the second
last state, and so on for a total of up to tmax updates. The
accumulated updates are applied in a single gradient step.
Asynchronous advantage actor-critic: The algorithm,
which we call asynchronous advantage actor-critic (A3C),
maintains a policy π(at|st; θ) and an estimate of the value
function V (st; θv). Like our variant of n-step Q-learning,
our variant of actor-critic also operates in the forward view
and uses the same mix of n-step returns to update both the
policy and the value-function. The policy and the value
function are updated after every tmax actions or when a
terminal state is reached. The update performed by the al-
gorithm can be seen as ∇θ(cid:48) log π(at|st; θ(cid:48))A(st, at; θ, θv)
where A(st, at; θ, θv) is an estimate of the advantage func-
i=0 γirt+i + γkV (st+k; θv) − V (st; θv),
where k can vary from state to state and is upper-bounded

tion given by(cid:80)k−1

by tmax. The pseudocode for the algorithm is presented in
Supplementary Algorithm S3.
As with the value-based methods we rely on parallel actor-
learners and accumulated updates for improving training
stability. Note that while the parameters θ of the policy
and θv of the value function are shown as being separate
for generality, we always share some of the parameters in
practice. We typically use a convolutional neural network
that has one softmax output for the policy π(at|st; θ) and
one linear output for the value function V (st; θv), with all
non-output layers shared.
We also found that adding the entropy of the policy π to the
objective function improved exploration by discouraging
premature convergence to suboptimal deterministic poli-
cies. This technique was originally proposed by (Williams
& Peng, 1991), who found that it was particularly help-
ful on tasks requiring hierarchical behavior. The gradi-
ent of the full objective function including the entropy
regularization term with respect to the policy parame-
ters takes the form ∇θ(cid:48) log π(at|st; θ(cid:48))(Rt − V (st; θv)) +
β∇θ(cid:48)H(π(st; θ(cid:48))), where H is the entropy. The hyperpa-
rameter β controls the strength of the entropy regulariza-
tion term.
Optimization: We investigated three different optimiza-
tion algorithms in our asynchronous framework – SGD
with momentum, RMSProp (Tieleman & Hinton, 2012)
without shared statistics, and RMSProp with shared statis-
tics. We used the standard non-centered RMSProp update
given by

g = αg + (1 − α)∆θ2 and θ ← θ − η

∆θ√
g + 

,

(1)

where all operations are performed elementwise. A com-
parison on a subset of Atari 2600 games showed that a vari-
ant of RMSProp where statistics g are shared across threads
is considerably more robust than the other two methods.
Full details of the methods and comparisons are included
in Supplementary Section 7.

5. Experiments
We use four different platforms for assessing the properties
of the proposed framework. We perform most of our exper-
iments using the Arcade Learning Environment (Bellemare
et al., 2012), which provides a simulator for Atari 2600
games. This is one of the most commonly used benchmark
environments for RL algorithms. We use the Atari domain
to compare against state of the art results (Van Hasselt et al.,
2015; Wang et al., 2015; Schaul et al., 2015; Nair et al.,
2015; Mnih et al., 2015), as well as to carry out a detailed
stability and scalability analysis of the proposed methods.
We performed further comparisons using the TORCS 3D
car racing simulator (Wymann et al., 2013). We also use

Asynchronous Methods for Deep Reinforcement Learning

Figure 1. Learning speed comparison for DQN and the new asynchronous algorithms on ﬁve Atari 2600 games. DQN was trained on
a single Nvidia K40 GPU while the asynchronous methods were trained using 16 CPU cores. The plots are averaged over 5 runs. In
the case of DQN the runs were for different seeds with ﬁxed hyperparameters. For asynchronous methods we average over the best 5
models from 50 experiments with learning rates sampled from LogU nif orm(10−4, 10−2) and all other hyperparameters ﬁxed.

two additional domains to evaluate only the A3C algorithm
– Mujoco and Labyrinth. MuJoCo (Todorov, 2015) is a
physics simulator for evaluating agents on continuous mo-
tor control tasks with contact dynamics. Labyrinth is a new
3D environment where the agent must learn to ﬁnd rewards
in randomly generated mazes from a visual input. The pre-
cise details of our experimental setup can be found in Sup-
plementary Section 8.

Method
DQN
Gorila
D-DQN
Dueling D-DQN
Prioritized DQN
A3C, FF
A3C, FF
A3C, LSTM

Training Time
8 days on GPU

4 days, 100 machines

8 days on GPU
8 days on GPU
8 days on GPU
1 day on CPU
4 days on CPU
4 days on CPU

Mean Median
121.9% 47.5%
215.2% 71.3%
332.9% 110.9%
343.8% 117.1%
463.6% 127.6%
344.1% 68.2%
496.8% 116.6%
623.0% 112.6%

5.1. Atari 2600 Games

We ﬁrst present results on a subset of Atari 2600 games to
demonstrate the training speed of the new methods. Fig-
ure 1 compares the learning speed of the DQN algorithm
trained on an Nvidia K40 GPU with the asynchronous
methods trained using 16 CPU cores on ﬁve Atari 2600
games. The results show that all four asynchronous meth-
ods we presented can successfully train neural network
controllers on the Atari domain. The asynchronous meth-
ods tend to learn faster than DQN, with signiﬁcantly faster
learning on some games, while training on only 16 CPU
cores. Additionally, the results suggest that n-step methods
learn faster than one-step methods on some games. Over-
all, the policy-based advantage actor-critic method signiﬁ-
cantly outperforms all three value-based methods.
We then evaluated asynchronous advantage actor-critic on
57 Atari games. In order to compare with the state of the
art in Atari game playing, we largely followed the train-
ing and evaluation protocol of (Van Hasselt et al., 2015).
Speciﬁcally, we tuned hyperparameters (learning rate and
amount of gradient norm clipping) using a search on six
Atari games (Beamrider, Breakout, Pong, Q*bert, Seaquest
and Space Invaders) and then ﬁxed all hyperparameters for
all 57 games. We trained both a feedforward agent with the
same architecture as (Mnih et al., 2015; Nair et al., 2015;
Van Hasselt et al., 2015) as well as a recurrent agent with an
additional 256 LSTM cells after the ﬁnal hidden layer. We
additionally used the ﬁnal network weights for evaluation
to make the results more comparable to the original results

Table 1. Mean and median human-normalized scores on 57 Atari
games using the human starts evaluation metric. Supplementary
Table SS3 shows the raw scores for all games.

from (Bellemare et al., 2012). We trained our agents for
four days using 16 CPU cores, while the other agents were
trained for 8 to 10 days on Nvidia K40 GPUs. Table 1
shows the average and median human-normalized scores
obtained by our agents trained by asynchronous advantage
actor-critic (A3C) as well as the current state-of-the art.
Supplementary Table S3 shows the scores on all games.
A3C signiﬁcantly improves on state-of-the-art the average
score over 57 games in half the training time of the other
methods while using only 16 CPU cores and no GPU. Fur-
thermore, after just one day of training, A3C matches the
average human normalized score of Dueling Double DQN
and almost reaches the median human normalized score of
Gorila. We note that many of the improvements that are
presented in Double DQN (Van Hasselt et al., 2015) and
Dueling Double DQN (Wang et al., 2015) can be incorpo-
rated to 1-step Q and n-step Q methods presented in this
work with similar potential improvements.

5.2. TORCS Car Racing Simulator

We also compared the four asynchronous methods on
the TORCS 3D car racing game (Wymann et al., 2013).
TORCS not only has more realistic graphics than Atari
2600 games, but also requires the agent to learn the dy-
namics of the car it is controlling. At each step, an agent
received only a visual input in the form of an RGB image

02468101214Training time (hours)0200040006000800010000120001400016000ScoreBeamriderDQN1-step Q1-step SARSAn-step QA3C02468101214Training time (hours)0100200300400500600ScoreBreakoutDQN1-step Q1-step SARSAn-step QA3C02468101214Training time (hours)3020100102030ScorePongDQN1-step Q1-step SARSAn-step QA3C02468101214Training time (hours)020004000600080001000012000ScoreQ*bertDQN1-step Q1-step SARSAn-step QA3C02468101214Training time (hours)02004006008001000120014001600ScoreSpace InvadersDQN1-step Q1-step SARSAn-step QA3CAsynchronous Methods for Deep Reinforcement Learning

of the current frame as well as a reward proportional to the
agent’s velocity along the center of the track at the agent’s
current position. We used the same neural network archi-
tecture as the one used in the Atari experiments speciﬁed in
Supplementary Section 8. We performed experiments us-
ing four different settings – the agent controlling a slow car
with and without opponent bots, and the agent controlling a
fast car with and without opponent bots. Full results can be
found in Supplementary Figure S6. A3C was the best per-
forming agent, reaching between roughly 75% and 90% of
the score obtained by a human tester on all four game con-
ﬁgurations in about 12 hours of training. A video showing
the learned driving behavior of the A3C agent can be found
at https://youtu.be/0xo1Ldx3L5Q.

5.3. Continuous Action Control Using the MuJoCo

Physics Simulator

We also examined a set of tasks where the action space
is continuous.
In particular, we looked at a set of rigid
body physics domains with contact dynamics where the
tasks include many examples of manipulation and loco-
motion. These tasks were simulated using the Mujoco
physics engine. We evaluated only the asynchronous ad-
vantage actor-critic algorithm since, unlike the value-based
methods, it is easily extended to continuous actions. In all
problems, using either the physical state or pixels as in-
put, Asynchronous Advantage-Critic found good solutions
in less than 24 hours of training and typically in under a few
hours. Some successful policies learned by our agent can
be seen in the following video https://youtu.be/
Ajjc08-iPx8. Further details about this experiment can
be found in Supplementary Section 9.

5.4. Labyrinth

We performed an additional set of experiments with A3C
on a new 3D environment called Labyrinth. The speciﬁc
task we considered involved the agent learning to ﬁnd re-
wards in randomly generated mazes. At the beginning of
each episode the agent was placed in a new randomly gen-
erated maze consisting of rooms and corridors. Each maze
contained two types of objects that the agent was rewarded
for ﬁnding – apples and portals. Picking up an apple led to
a reward of 1. Entering a portal led to a reward of 10 after
which the agent was respawned in a new random location in
the maze and all previously collected apples were regener-
ated. An episode terminated after 60 seconds after which a
new episode would begin. The aim of the agent is to collect
as many points as possible in the time limit and the optimal
strategy involves ﬁrst ﬁnding the portal and then repeatedly
going back to it after each respawn. This task is much more
challenging than the TORCS driving domain because the
agent is faced with a new maze in each episode and must
learn a general strategy for exploring random mazes.

1
Method
1-step Q
1.0
1-step SARSA 1.0
1.0
n-step Q
A3C
1.0

Number of threads
2
3.0
2.8
2.7
2.1

8
13.3
13.1
10.7
6.9

4
6.3
5.9
5.9
3.7

16
24.1
22.1
17.2
12.5

Table 2. The average training speedup for each method and num-
ber of threads averaged over seven Atari games. To compute the
training speed-up on a single game we measured the time to re-
quired reach a ﬁxed reference score using each method and num-
ber of threads. The speedup from using n threads on a game was
deﬁned as the time required to reach a ﬁxed reference score using
one thread divided the time required to reach the reference score
using n threads. The table shows the speedups averaged over
seven Atari games (Beamrider, Breakout, Enduro, Pong, Q*bert,
Seaquest, and Space Invaders).

We trained an A3C LSTM agent on this task using only
84 × 84 RGB images as input. The ﬁnal average score
of around 50 indicates that the agent learned a reason-
able strategy for exploring random 3D maxes using only
a visual input. A video showing one of the agents ex-
ploring previously unseen mazes is included at https:
//youtu.be/nMR5mjCFZCw.

5.5. Scalability and Data Efﬁciency

We analyzed the effectiveness of our proposed framework
by looking at how the training time and data efﬁciency
changes with the number of parallel actor-learners. When
using multiple workers in parallel and updating a shared
model, one would expect that in an ideal case, for a given
task and algorithm, the number of training steps to achieve
a certain score would remain the same with varying num-
bers of workers. Therefore, the advantage would be solely
due to the ability of the system to consume more data in
the same amount of wall clock time and possibly improved
exploration. Table 2 shows the training speed-up achieved
by using increasing numbers of parallel actor-learners av-
eraged over seven Atari games. These results show that all
four methods achieve substantial speedups from using mul-
tiple worker threads, with 16 threads leading to at least an
order of magnitude speedup. This conﬁrms that our pro-
posed framework scales well with the number of parallel
workers, making efﬁcient use of resources.
Somewhat surprisingly, asynchronous one-step Q-learning
and Sarsa algorithms exhibit superlinear speedups that
cannot be explained by purely computational gains. We
observe that one-step methods (one-step Q and one-step
Sarsa) often require less data to achieve a particular score
when using more parallel actor-learners. We believe this
is due to positive effect of multiple threads to reduce the
bias in one-step methods. These effects are shown more
clearly in Figure 3, which shows plots of the average score
against the total number of training frames for different

Asynchronous Methods for Deep Reinforcement Learning

Figure 2. Scatter plots of scores obtained by asynchronous advantage actor-critic on ﬁve games (Beamrider, Breakout, Pong, Q*bert,
Space Invaders) for 50 different learning rates and random initializations. On each game, there is a wide range of learning rates for
which all random initializations acheive good scores. This shows that A3C is quite robust to learning rates and initial random weights.

numbers of actor-learners and training methods on ﬁve
Atari games, and Figure 4, which shows plots of the av-
erage score against wall-clock time.

5.6. Robustness and Stability

Finally, we analyzed the stability and robustness of the
four proposed asynchronous algorithms. For each of the
four algorithms we trained models on ﬁve games (Break-
out, Beamrider, Pong, Q*bert, Space Invaders) using 50
different learning rates and random initializations. Figure 2
shows scatter plots of the resulting scores for A3C, while
Supplementary Figure S11 shows plots for the other three
methods. There is usually a range of learning rates for each
method and game combination that leads to good scores,
indicating that all methods are quite robust to the choice of
learning rate and random initialization. The fact that there
are virtually no points with scores of 0 in regions with good
learning rates indicates that the methods are stable and do
not collapse or diverge once they are learning.

6. Conclusions and Discussion
We have presented asynchronous versions of four standard
reinforcement learning algorithms and showed that they
are able to train neural network controllers on a variety
of domains in a stable manner. Our results show that in
our proposed framework stable training of neural networks
through reinforcement learning is possible with both value-
based and policy-based methods, off-policy as well as on-
policy methods, and in discrete as well as continuous do-
mains. When trained on the Atari domain using 16 CPU
cores, the proposed asynchronous algorithms train faster
than DQN trained on an Nvidia K40 GPU, with A3C sur-
passing the current state-of-the-art in half the training time.
One of our main ﬁndings is that using parallel actor-
learners to update a shared model had a stabilizing effect on
the learning process of the three value-based methods we
considered. While this shows that stable online Q-learning
is possible without experience replay, which was used for
this purpose in DQN, it does not mean that experience re-
play is not useful.
Incorporating experience replay into
the asynchronous reinforcement learning framework could

substantially improve the data efﬁciency of these methods
by reusing old data. This could in turn lead to much faster
training times in domains like TORCS where interacting
with the environment is more expensive than updating the
model for the architecture we used.
Combining other existing reinforcement learning meth-
ods or recent advances in deep reinforcement learning
with our asynchronous framework presents many possibil-
ities for immediate improvements to the methods we pre-
sented. While our n-step methods operate in the forward
view (Sutton & Barto, 1998) by using corrected n-step re-
turns directly as targets, it has been more common to use
the backward view to implicitly combine different returns
through eligibility traces (Watkins, 1989; Sutton & Barto,
1998; Peng & Williams, 1996). The asynchronous ad-
vantage actor-critic method could be potentially improved
by using other ways of estimating the advantage function,
such as generalized advantage estimation of (Schulman
et al., 2015b). All of the value-based methods we inves-
tigated could beneﬁt from different ways of reducing over-
estimation bias of Q-values (Van Hasselt et al., 2015; Belle-
mare et al., 2016). Yet another, more speculative, direction
is to try and combine the recent work on true online tempo-
ral difference methods (van Seijen et al., 2015) with non-
linear function approximation.
In addition to these algorithmic improvements, a number
of complementary improvements to the neural network ar-
chitecture are possible. The dueling architecture of (Wang
et al., 2015) has been shown to produce more accurate es-
timates of Q-values by including separate streams for the
state value and advantage in the network. The spatial soft-
max proposed by (Levine et al., 2015) could improve both
value-based and policy-based methods by making it easier
for the network to represent feature coordinates.

ACKNOWLEDGMENTS

We thank Thomas Degris, Remi Munos, Marc Lanctot,
Sasha Vezhnevets and Joseph Modayil for many helpful
discussions, suggestions and comments on the paper. We
also thank the DeepMind evaluation team for setting up the
environments used to evaluate the agents in the paper.

10-410-310-2Learning rate20000200040006000800010000120001400016000ScoreA3C, Beamrider10-410-310-2Learning rate20002004006008001000ScoreA3C, Breakout10-410-310-2Learning rate3020100102030ScoreA3C, Pong10-410-310-2Learning rate2000020004000600080001000012000ScoreA3C, Q*bert10-410-310-2Learning rate0200400600800100012001400ScoreA3C, Space InvadersAsynchronous Methods for Deep Reinforcement Learning

Figure 3. Data efﬁciency comparison of different numbers of actor-learners for three asynchronous methods on ﬁve Atari games. The
x-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads). The y-axis
shows the average score. Each curve shows the average over the three best learning rates. Single step methods show increased data
efﬁciency from more parallel workers. Results for Sarsa are shown in Supplementary Figure S9.

Figure 4. Training speed comparison of different numbers of actor-learners on ﬁve Atari games. The x-axis shows training time in
hours while the y-axis shows the average score. Each curve shows the average over the three best learning rates. All asynchronous
methods show signiﬁcant speedups from using greater numbers of parallel actor-learners. Results for Sarsa are shown in Supplementary
Figure S10.

010203040Training epochs0200040006000800010000ScoreBeamrider1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads010203040Training epochs050100150200250300350ScoreBreakout1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads010203040Training epochs25201510505101520ScorePong1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads010203040Training epochs050010001500200025003000350040004500ScoreQ*bert1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads010203040Training epochs100200300400500600700800ScoreSpace Invaders1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads010203040Training epochs020004000600080001000012000ScoreBeamridern-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads010203040Training epochs050100150200250300350ScoreBreakoutn-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads010203040Training epochs25201510505101520ScorePongn-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads010203040Training epochs0100020003000400050006000ScoreQ*bertn-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads010203040Training epochs100200300400500600700800ScoreSpace Invadersn-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads010203040Training epochs0200040006000800010000120001400016000ScoreBeamriderA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threads010203040Training epochs0100200300400500600700800ScoreBreakoutA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threads010203040Training epochs3020100102030ScorePongA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threads010203040Training epochs020004000600080001000012000ScoreQ*bertA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threads010203040Training epochs0200400600800100012001400ScoreSpace InvadersA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threads02468101214Training time (hours)0100020003000400050006000700080009000ScoreBeamrider1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads02468101214Training time (hours)050100150200250300ScoreBreakout1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads02468101214Training time (hours)25201510505101520ScorePong1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads02468101214Training time (hours)05001000150020002500300035004000ScoreQ*bert1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads02468101214Training time (hours)100200300400500600700800ScoreSpace Invaders1-step Q, 1 threads1-step Q, 2 threads1-step Q, 4 threads1-step Q, 8 threads1-step Q, 16 threads02468101214Training time (hours)020004000600080001000012000ScoreBeamridern-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads02468101214Training time (hours)050100150200250300350ScoreBreakoutn-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads02468101214Training time (hours)25201510505101520ScorePongn-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads02468101214Training time (hours)050010001500200025003000350040004500ScoreQ*bertn-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads02468101214Training time (hours)100200300400500600700800ScoreSpace Invadersn-step Q, 1 threadsn-step Q, 2 threadsn-step Q, 4 threadsn-step Q, 8 threadsn-step Q, 16 threads02468101214Training time (hours)0200040006000800010000120001400016000ScoreBeamriderA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threads02468101214Training time (hours)0100200300400500600ScoreBreakoutA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threads02468101214Training time (hours)3020100102030ScorePongA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threads02468101214Training time (hours)020004000600080001000012000ScoreQ*bertA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threads02468101214Training time (hours)02004006008001000120014001600ScoreSpace InvadersA3C, 1 threadsA3C, 2 threadsA3C, 4 threadsA3C, 8 threadsA3C, 16 threadsAsynchronous Methods for Deep Reinforcement Learning

References
Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and
Bowling, Michael. The arcade learning environment:
An evaluation platform for general agents. Journal of
Artiﬁcial Intelligence Research, 2012.

Bellemare, Marc G., Ostrovski, Georg, Guez, Arthur,
Thomas, Philip S., and Munos, Rémi. Increasing the ac-
tion gap: New operators for reinforcement learning. In
Proceedings of the AAAI Conference on Artiﬁcial Intel-
ligence, 2016.

Bertsekas, Dimitri P. Distributed dynamic programming.
Automatic Control, IEEE Transactions on, 27(3):610–
616, 1982.

Chavez, Kevin, Ong, Hao Yi, and Hong, Augustus. Dis-
tributed deep q-learning. Technical report, Stanford Uni-
versity, June 2015.

Degris, Thomas, Pilarski, Patrick M, and Sutton, Richard S.
Model-free reinforcement learning with continuous ac-
tion in practice. In American Control Conference (ACC),
2012, pp. 2177–2182. IEEE, 2012.

Grounds, Matthew and Kudenko, Daniel. Parallel rein-
forcement learning with linear function approximation.
In Proceedings of the 5th, 6th and 7th European Confer-
ence on Adaptive and Learning Agents and Multi-agent
Systems: Adaptation and Multi-agent Learning, pp. 60–
74. Springer-Verlag, 2008.

Koutník, Jan, Schmidhuber, Jürgen, and Gomez, Faustino.
Evolving deep unsupervised convolutional networks for
vision-based reinforcement learning. In Proceedings of
the 2014 conference on Genetic and evolutionary com-
putation, pp. 541–548. ACM, 2014.

Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,
Pieter. End-to-end training of deep visuomotor policies.
arXiv preprint arXiv:1504.00702, 2015.

Li, Yuxi and Schuurmans, Dale. Mapreduce for parallel re-
inforcement learning. In Recent Advances in Reinforce-
ment Learning - 9th European Workshop, EWRL 2011,
Athens, Greece, September 9-11, 2011, Revised Selected
Papers, pp. 309–320, 2011.

Lillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander,
Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,
and Wierstra, Daan. Continuous control with deep re-
inforcement learning. arXiv preprint arXiv:1509.02971,
2015.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Graves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and
Riedmiller, Martin. Playing atari with deep reinforce-
ment learning. In NIPS Deep Learning Workshop. 2013.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Rusu, Andrei A., Veness, Joel, Bellemare, Marc G.,
Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K.,
Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,
Amir, Antonoglou, Ioannis, King, Helen, Kumaran,
Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis,
Demis. Human-level control through deep reinforcement
learning. Nature, 518(7540):529–533, 02 2015. URL
http://dx.doi.org/10.1038/nature14236.

Nair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alci-
cek, Cagdas, Fearon, Rory, Maria, Alessandro De, Pan-
neershelvam, Vedavyas, Suleyman, Mustafa, Beattie,
Charles, Petersen, Stig, Legg, Shane, Mnih, Volodymyr,
Kavukcuoglu, Koray, and Silver, David. Massively par-
allel methods for deep reinforcement learning. In ICML
Deep Learning Workshop. 2015.

Peng, Jing and Williams, Ronald J. Incremental multi-step
q-learning. Machine Learning, 22(1-3):283–290, 1996.

Recht, Benjamin, Re, Christopher, Wright, Stephen, and
Niu, Feng. Hogwild: A lock-free approach to paralleliz-
ing stochastic gradient descent. In Advances in Neural
Information Processing Systems, pp. 693–701, 2011.

Riedmiller, Martin. Neural ﬁtted q iteration–ﬁrst experi-
ences with a data efﬁcient neural reinforcement learning
method. In Machine Learning: ECML 2005, pp. 317–
328. Springer Berlin Heidelberg, 2005.

Rummery, Gavin A and Niranjan, Mahesan. On-line q-

learning using connectionist systems. 1994.

Schaul, Tom, Quan, John, Antonoglou, Ioannis, and Sil-
ver, David. Prioritized experience replay. arXiv preprint
arXiv:1511.05952, 2015.

Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan,
Michael I, and Abbeel, Pieter. Trust region policy op-
In International Conference on Machine
timization.
Learning (ICML), 2015a.

Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan,
Michael, and Abbeel, Pieter. High-dimensional con-
tinuous control using generalized advantage estimation.
arXiv preprint arXiv:1506.02438, 2015b.

Sutton, R. and Barto, A. Reinforcement Learning: an In-

troduction. MIT Press, 1998.

Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-
rmsprop: Divide the gradient by a running average of
its recent magnitude. COURSERA: Neural Networks for
Machine Learning, 4, 2012.

Todorov, E. MuJoCo: Modeling, Simulation and Visual-
ization of Multi-Joint Dynamics with Contact (ed 1.0).
Roboti Publishing, 2015.

Asynchronous Methods for Deep Reinforcement Learning

Tomassini, Marco. Parallel and distributed evolutionary al-

gorithms: A review. Technical report, 1999.

Tsitsiklis, John N. Asynchronous stochastic approxima-
tion and q-learning. Machine Learning, 16(3):185–202,
1994.

Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep
reinforcement learning with double q-learning. arXiv
preprint arXiv:1509.06461, 2015.

van Seijen, H., Rupam Mahmood, A., Pilarski, P. M.,
Machado, M. C., and Sutton, R. S.
True Online
Temporal-Difference Learning. ArXiv e-prints, Decem-
ber 2015.

Wang, Z., de Freitas, N., and Lanctot, M. Dueling Network
Architectures for Deep Reinforcement Learning. ArXiv
e-prints, November 2015.

Watkins, Christopher John Cornish Hellaby. Learning from
delayed rewards. PhD thesis, University of Cambridge
England, 1989.

Williams, R.J. Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Ma-
chine Learning, 8(3):229–256, 1992.

Williams, Ronald J and Peng, Jing. Function optimization
using connectionist reinforcement learning algorithms.
Connection Science, 3(3):241–268, 1991.

Wymann, B., EspiÃl’, E., Guionneau, C., Dimitrakakis, C.,
Coulom, R., and Sumner, A. Torcs: The open racing car
simulator, v1.3.5, 2013.

Supplementary Material for "Asynchronous Methods for Deep

Reinforcement Learning"

June 17, 2016

7. Optimization Details
We investigated two different optimization algorithms with our asynchronous framework – stochastic gradient
descent and RMSProp. Our implementations of these algorithms do not use any locking in order to maximize
throughput when using a large number of threads.
Momentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and
well studied (Recht et al., 2011). Let θ be the parameter vector that is shared across all threads and let ∆θi
be the accumulated gradients of the loss with respect to parameters θ computed by thread number i. Each
thread i independently applies the standard momentum SGD update mi = αmi + (1 − α)∆θi followed by
θ ← θ − ηmi with learning rate η, momentum α and without any locks. Note that in this setting, each thread
maintains its own separate gradient and momentum vector.
RMSProp: While RMSProp (Tieleman & Hinton, 2012) has been widely used in the deep learning literature,
it has not been extensively studied in the asynchronous optimization setting. The standard non-centered
RMSProp update is given by

g = αg + (1 − α)∆θ2

θ ← θ − η

∆θ√
g + 

,

(S2)

(S3)

where all operations are performed elementwise. In order to apply RMSProp in the asynchronous optimiza-
tion setting one must decide whether the moving average of elementwise squared gradients g is shared or
per-thread. We experimented with two versions of the algorithm. In one version, which we refer to as RM-
SProp, each thread maintains its own g shown in Equation S2. In the other version, which we call Shared
RMSProp, the vector g is shared among threads and is updated asynchronously and without locking. Sharing
statistics among threads also reduces memory requirements by using one fewer copy of the parameter vector
per thread.
We compared these three asynchronous optimization algorithms in terms of their sensitivity to different learn-
ing rates and random network initializations. Figure S5 shows a comparison of the methods for two different
reinforcement learning methods (Async n-step Q and Async Advantage Actor-Critic) on four different games
(Breakout, Beamrider, Seaquest and Space Invaders). Each curve shows the scores for 50 experiments that
correspond to 50 different random learning rates and initializations. The x-axis shows the rank of the model
after sorting in descending order by ﬁnal average score and the y-axis shows the ﬁnal average score achieved
by the corresponding model. In this representation, the algorithm that performs better would achieve higher
maximum rewards on the y-axis and the algorithm that is most robust would have its slope closest to horizon-
tal, thus maximizing the area under the curve. RMSProp with shared statistics tends to be more robust than
RMSProp with per-thread statistics, which is in turn more robust than Momentum SGD.

Asynchronous Methods for Deep Reinforcement Learning

8. Experimental Setup
The experiments performed on a subset of Atari games (Figures 1, 3, 4 and Table 2) as well as the TORCS
experiments (Figure S6) used the following setup. Each experiment used 16 actor-learner threads running
on a single machine and no GPUs. All methods performed updates after every 5 actions (tmax = 5 and
IU pdate = 5) and shared RMSProp was used for optimization. The three asynchronous value-based methods
used a shared target network that was updated every 40000 frames. The Atari experiments used the same
input preprocessing as (Mnih et al., 2015) and an action repeat of 4. The agents used the network architecture
from (Mnih et al., 2013). The network used a convolutional layer with 16 ﬁlters of size 8 × 8 with stride
4, followed by a convolutional layer with with 32 ﬁlters of size 4 × 4 with stride 2, followed by a fully
connected layer with 256 hidden units. All three hidden layers were followed by a rectiﬁer nonlinearity. The
value-based methods had a single linear output unit for each action representing the action-value. The model
used by actor-critic agents had two set of outputs – a softmax output with one entry per action representing the
probability of selecting the action, and a single linear output representing the value function. All experiments
used a discount of γ = 0.99 and an RMSProp decay factor of α = 0.99.
The value based methods sampled the exploration rate  from a distribution taking three values 1, 2, 3 with
probabilities 0.4, 0.3, 0.3. The values of 1, 2, 3 were annealed from 1 to 0.1, 0.01, 0.5 respectively over
the ﬁrst four million frames. Advantage actor-critic used entropy regularization with a weight β = 0.01 for
all Atari and TORCS experiments. We performed a set of 50 experiments for ﬁve Atari games and every
TORCS level, each using a different random initialization and initial learning rate. The initial learning rate
was sampled from a LogU nif orm(10−4, 10−2) distribution and annealed to 0 over the course of training.
Note that in comparisons to prior work (Tables 1 and S3) we followed standard evaluation protocol and used
ﬁxed hyperparameters.

9. Continuous Action Control Using the MuJoCo Physics Simulator
To apply the asynchronous advantage actor-critic algorithm to the Mujoco tasks the necessary setup is nearly
identical to that used in the discrete action domains, so here we enumerate only the differences required for
the continuous action domains. The essential elements for many of the tasks (i.e. the physics models and
task objectives) are near identical to the tasks examined in (Lillicrap et al., 2015). However, the rewards and
thus performance are not comparable for most of the tasks due to changes made by the developers of Mujoco
which altered the contact model.
For all the domains we attempted to learn the task using the physical state as input. The physical state
consisted of the joint positions and velocities as well as the target position if the task required a target. In
addition, for three of the tasks (pendulum, pointmass2D, and gripper) we also examined training directly from
RGB pixel inputs. In the low dimensional physical state case, the inputs are mapped to a hidden state using
one hidden layer with 200 ReLU units. In the cases where we used pixels, the input was passed through two
layers of spatial convolutions without any non-linearity or pooling. In either case, the output of the encoder
layers were fed to a single layer of 128 LSTM cells. The most important difference in the architecture is in the
the output layer of the policy network. Unlike the discrete action domain where the action output is a Softmax,
here the two outputs of the policy network are two real number vectors which we treat as the mean vector µ
and scalar variance σ2 of a multidimensional normal distribution with a spherical covariance. To act, the input
is passed through the model to the output layer where we sample from the normal distribution determined by
µ and σ2. In practice, µ is modeled by a linear layer and σ2 by a SoftPlus operation, log(1 + exp(x)), as the
activation computed as a function of the output of a linear layer. In our experiments with continuous control
problems the networks for policy network and value network do not share any parameters, though this detail
is unlikely to be crucial. Finally, since the episodes were typically at most several hundred time steps long,
we did not use any bootstrapping in the policy or value function updates and batched each episode into a
single update.
As in the discrete action case, we included an entropy cost which encouraged exploration. In the continuous

Asynchronous Methods for Deep Reinforcement Learning

case the we used a cost on the differential entropy of the normal distribution deﬁned by the output of the
actor network, − 1
2 (log(2πσ2) + 1), we used a constant multiplier of 10−4 for this cost across all of the tasks
examined. The asynchronous advantage actor-critic algorithm ﬁnds solutions for all the domains. Figure S8
shows learning curves against wall-clock time, and demonstrates that most of the domains from states can be
solved within a few hours. All of the experiments, including those done from pixel based observations, were
run on CPU. Even in the case of solving the domains directly from pixel inputs we found that it was possible
to reliably discover solutions within 24 hours. Figure S7 shows scatter plots of the top scores against the
sampled learning rates. In most of the domains there is large range of learning rates that consistently achieve
good performance on the task.

Algorithm S2 Asynchronous n-step Q-learning - pseudocode for each actor-learner thread.

// Assume global shared parameter vector θ.
// Assume global shared target parameter vector θ−.
// Assume global shared counter T = 0.
Initialize thread step counter t ← 1
Initialize target network parameters θ− ← θ
Initialize thread-speciﬁc parameters θ(cid:48) = θ
Initialize network gradients dθ ← 0
repeat

Clear gradients dθ ← 0
Synchronize thread-speciﬁc parameters θ(cid:48) = θ
tstart = t
Get state st
repeat

Take action at according to the -greedy policy based on Q(st, a; θ(cid:48))
Receive reward rt and new state st+1
t ← t + 1
T ← T + 1

(cid:26) 0

for terminal st
for non-terminal st

until terminal st or t − tstart == tmax
R =
for i ∈ {t − 1, . . . , tstart} do

maxa Q(st, a; θ−)

R ← ri + γR
Accumulate gradients wrt θ(cid:48): dθ ← dθ +

end for
Perform asynchronous update of θ using dθ.
if T mod Itarget == 0 then

∂(R−Q(si,ai;θ(cid:48)))2

∂θ(cid:48)

θ− ← θ

end if

until T > Tmax

Asynchronous Methods for Deep Reinforcement Learning

Algorithm S3 Asynchronous advantage actor-critic - pseudocode for each actor-learner thread.

// Assume global shared parameter vectors θ and θv and global shared counter T = 0
// Assume thread-speciﬁc parameter vectors θ(cid:48) and θ(cid:48)
Initialize thread step counter t ← 1
repeat

v

Reset gradients: dθ ← 0 and dθv ← 0.
Synchronize thread-speciﬁc parameters θ(cid:48) = θ and θ(cid:48)
tstart = t
Get state st
repeat

Perform at according to policy π(at|st; θ(cid:48))
Receive reward rt and new state st+1
t ← t + 1
T ← T + 1

v = θv

(cid:26) 0

until terminal st or t − tstart == tmax
R =
for i ∈ {t − 1, . . . , tstart} do

for terminal st
for non-terminal st// Bootstrap from last state

V (st, θ(cid:48)
v)
R ← ri + γR
Accumulate gradients wrt θ(cid:48): dθ ← dθ + ∇θ(cid:48) log π(ai|si; θ(cid:48))(R − V (si; θ(cid:48)
Accumulate gradients wrt θ(cid:48)

v

v: dθv ← dθv + ∂ (R − V (si; θ(cid:48)

v))2/∂θ(cid:48)

end for
Perform asynchronous update of θ using dθ and of θv using dθv.

v))

until T > Tmax

Asynchronous Methods for Deep Reinforcement Learning

Figure S5. Comparison of three different optimization methods (Momentum SGD, RMSProp, Shared RMSProp) tested
using two different algorithms (Async n-step Q and Async Advantage Actor-Critic) on four different Atari games (Break-
out, Beamrider, Seaquest and Space Invaders). Each curve shows the ﬁnal scores for 50 experiments sorted in descending
order that covers a search over 50 random initializations and learning rates. The top row shows results using Async n-step
Q algorithm and bottom row shows results with Async Advantage Actor-Critic. Each individual graph shows results for
one of the four games and three different optimization methods. Shared RMSProp tends to be more robust to different
learning rates and random initializations than Momentum SGD and RMSProp without sharing.

Figure S6. Comparison of algorithms on the TORCS car racing simulator. Four different conﬁgurations of car speed and
opponent presence or absence are shown. In each plot, all four algorithms (one-step Q, one-step Sarsa, n-step Q and
Advantage Actor-Critic) are compared on score vs training time in wall clock hours. Multi-step algorithms achieve better
policies much faster than one-step algorithms on all four levels. The curves show averages over the 5 best runs from 50
experiments with learning rates sampled from LogU nif orm(10−4, 10−2) and all other hyperparameters ﬁxed.

1020304050Model Rank050100150200250300350400ScoreBreakoutn-step Q, SGDn-step Q, RMSPropn-step Q, Shared RMSProp1020304050Model Rank0500010000150002000025000ScoreBeamridern-step Q, SGDn-step Q, RMSPropn-step Q, Shared RMSProp1020304050Model Rank0100020003000400050006000ScoreSeaquestn-step Q, SGDn-step Q, RMSPropn-step Q, Shared RMSProp1020304050Model Rank020040060080010001200140016001800ScoreSpace Invadersn-step Q, SGDn-step Q, RMSPropn-step Q, Shared RMSProp1020304050Model Rank0100200300400500600700800900ScoreBreakoutA3C, SGDA3C, RMSPropA3C, Shared RMSProp1020304050Model Rank0500010000150002000025000ScoreBeamriderA3C, SGDA3C, RMSPropA3C, Shared RMSProp1020304050Model Rank20040060080010001200140016001800ScoreSeaquestA3C, SGDA3C, RMSPropA3C, Shared RMSProp1020304050Model Rank05001000150020002500300035004000ScoreSpace InvadersA3C, SGDA3C, RMSPropA3C, Shared RMSProp010203040Training time (hours)1000010002000300040005000ScoreSlow car, no botsAsync 1-step QAsync SARSAAsync n-step QAsync actor-criticHuman tester010203040Training time (hours)1000010002000300040005000ScoreSlow car, botsAsync 1-step QAsync SARSAAsync n-step QAsync actor-criticHuman tester010203040Training time (hours)10000100020003000400050006000ScoreFast car, no botsAsync 1-step QAsync SARSAAsync n-step QAsync actor-criticHuman tester010203040Training time (hours)10000100020003000400050006000ScoreFast car, botsAsync 1-step QAsync SARSAAsync n-step QAsync actor-criticHuman testerAsynchronous Methods for Deep Reinforcement Learning

Figure S7. Performance for the Mujoco continuous action domains. Scatter plot of the best score obtained against
learning rates sampled from LogU nif orm(10−5, 10−1). For nearly all of the tasks there is a wide range of learning
rates that lead to good performance on the task.

Asynchronous Methods for Deep Reinforcement Learning

Figure S8. Score per episode vs wall-clock time plots for the Mujoco domains. Each plot shows error bars for the top 5
experiments.

Figure S9. Data efﬁciency comparison of different numbers of actor-learners one-step Sarsa on ﬁve Atari games. The
x-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads).
The y-axis shows the average score. Each curve shows the average of the three best performing agents from a search over
50 random learning rates. Sarsa shows increased data efﬁciency with increased numbers of parallel workers.

010203040Training epochs020004000600080001000012000ScoreBeamrider1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threads010203040Training epochs050100150200250300350ScoreBreakout1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threads010203040Training epochs25201510505101520ScorePong1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threads010203040Training epochs050010001500200025003000350040004500ScoreQ*bert1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threads010203040Training epochs100200300400500600700800900ScoreSpace Invaders1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threadsAsynchronous Methods for Deep Reinforcement Learning

Figure S10. Training speed comparison of different numbers of actor-learners for all one-step Sarsa on ﬁve Atari games.
The x-axis shows training time in hours while the y-axis shows the average score. Each curve shows the average of the
three best performing agents from a search over 50 random learning rates. Sarsa shows signiﬁcant speedups from using
greater numbers of parallel actor-learners.

Figure S11. Scatter plots of scores obtained by one-step Q, one-step Sarsa, and n-step Q on ﬁve games (Beamrider,
Breakout, Pong, Q*bert, Space Invaders) for 50 different learning rates and random initializations. All algorithms exhibit
some level of robustness to the choice of learning rate.

02468101214Training time (hours)020004000600080001000012000ScoreBeamrider1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threads02468101214Training time (hours)050100150200250300350ScoreBreakout1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threads02468101214Training time (hours)25201510505101520ScorePong1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threads02468101214Training time (hours)0500100015002000250030003500ScoreQ*bert1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threads02468101214Training time (hours)100200300400500600700800ScoreSpace Invaders1-step SARSA, 1 threads1-step SARSA, 2 threads1-step SARSA, 4 threads1-step SARSA, 8 threads1-step SARSA, 16 threads10-410-310-2Learning rate020004000600080001000012000Score1-step Q, Beamrider10-410-310-2Learning rate50050100150200250300350400Score1-step Q, Breakout10-410-310-2Learning rate3020100102030Score1-step Q, Pong10-410-310-2Learning rate1000010002000300040005000Score1-step Q, Q*bert10-410-310-2Learning rate100200300400500600700800Score1-step Q, Space Invaders10-410-310-2Learning rate200002000400060008000100001200014000Score1-step SARSA, Beamrider10-410-310-2Learning rate50050100150200250300350400Score1-step SARSA, Breakout10-410-310-2Learning rate25201510505101520Score1-step SARSA, Pong10-410-310-2Learning rate1000010002000300040005000Score1-step SARSA, Q*bert10-410-310-2Learning rate100200300400500600700800900Score1-step SARSA, Space Invaders10-410-310-2Learning rate20000200040006000800010000120001400016000Scoren-step Q, Beamrider10-410-310-2Learning rate50050100150200250300350400Scoren-step Q, Breakout10-410-310-2Learning rate3020100102030Scoren-step Q, Pong10-410-310-2Learning rate1000010002000300040005000Scoren-step Q, Q*bert10-410-310-2Learning rate3004005006007008009001000Scoren-step Q, Space InvadersAsynchronous Methods for Deep Reinforcement Learning

Game
Alien
Amidar
Assault
Asterix
Asteroids
Atlantis
Bank Heist
Battle Zone
Beam Rider
Berzerk
Bowling
Boxing
Breakout
Centipede
Chopper Comman
Crazy Climber
Defender
Demon Attack
Double Dunk
Enduro
Fishing Derby
Freeway
Frostbite
Gopher
Gravitar
H.E.R.O.
Ice Hockey
James Bond
Kangaroo
Krull
Kung-Fu Master
Montezuma’s Revenge
Ms. Pacman
Name This Game
Phoenix
Pit Fall
Pong
Private Eye
Q*Bert
River Raid
Road Runner
Robotank
Seaquest
Skiing
Solaris
Space Invaders
Star Gunner
Surround
Tennis
Time Pilot
Tutankham
Up and Down
Venture
Video Pinball
Wizard of Wor
Yars Revenge
Zaxxon

DQN
570.2
133.4
3332.3
124.5
697.1
76108.0
176.3
17560.0
8672.4

41.2
25.8
303.9
3773.1
3046.0
50992.0

12835.2
-21.6
475.6
-2.3
25.8
157.4
2731.8
216.5
12952.5
-3.8
348.5
2696.0
3864.0
11875.0
50.0
763.5
5439.9

16.2
298.2
4589.8
4065.3
9264.0
58.5
2793.9

1449.7
34081.0

-2.3
5640.0
32.4
3311.3
54.0
20228.1
246.0

Gorila
813.5
189.2
1195.8
3324.7
933.6
629166.5
399.4
19938.0
3822.1

54.0
74.2
313.0
6296.9
3191.8
65451.0

14880.1
-11.3
71.0
4.6
10.2
426.6
4373.0
538.4
8963.4
-1.7
444.0
1431.0
6363.1
20620.0
84.0
1263.0
9238.5

16.7
2598.6
7089.8
5310.3
43079.8
61.8
10145.9

1183.3
14919.2

-0.7
8267.8
118.5
8747.7
523.4
112093.4
10431.0

831.0

6159.4

Double
1033.4
169.1
6060.8
16837.0
1193.2
319688.0
886.0
24740.0
17417.2
1011.1
69.6
73.5
368.9
3853.5
3495.0
113782.0
27510.0
69803.4
-0.3
1216.6
3.2
28.8
1448.1
15253.0
200.5
14892.5
-2.5
573.0
11204.0
6796.1
30207.0
42.0
1241.3
8960.3
12366.5
-186.7
19.1
-575.5
11020.8
10838.4
43156.0
59.1
14498.0
-11490.4
810.0
2628.7
58365.0
1.9
-7.8
6608.0
92.2
19086.9
21.0
367823.7
6201.0
6270.6
8593.0

Dueling
1486.5
172.7
3994.8
15840.0
2035.4
445360.0
1129.3
31320.0
14591.3
910.6
65.7
77.3
411.6
4881.0
3784.0
124566.0
33996.0
56322.8
-0.8
2077.4
-4.1
0.2
2332.4
20051.4
297.0
15207.9
-1.3
835.5
10334.0
8051.6
24288.0
22.0
2250.6
11185.1
20410.5
-46.9
18.8
292.6
14175.8
16569.4
58549.0
62.0
37361.6
-11928.0
1768.4
5993.1
90804.0
4.0
4.4
6601.0
48.0
24759.2
200.0
110976.2
7054.0
25976.5
10164.0

Prioritized
900.5
218.4
7748.5
31907.5
1654.0
593642.0
816.8
29100.0
26172.7
1165.6
65.8
68.6
371.6
3421.9
6604.0
131086.0
21093.5
73185.8
2.7
1884.4
9.2
27.9
2930.2
57783.8
218.0
20506.4
-1.0
3511.5
10241.0
7406.5
31244.0
13.0
1824.6
11836.1
27430.1
-14.8
18.9
179.0
11277.0
18184.4
56990.0
55.4
39096.7
-10852.8
2238.2
9063.0
51959.0
-0.9
-2.0
7448.0
33.6
29443.7
244.0
374886.9
7451.0
5965.1
9501.0

A3C FF, 1 day
182.1
283.9
3746.1
6723.0
3009.4
772392.0
946.0
11340.0
13235.9
1433.4
36.2
33.7
551.6
3306.5
4669.0
101624.0
36242.5
84997.5
0.1
-82.2
13.6
0.1
180.1
8442.8
269.5
28765.8
-4.7
351.5
106.0
8066.6
3046.0
53.0
594.4
5614.0
28181.8
-123.0
11.4
194.4
13752.3
10001.2
31769.0
2.3
2300.2
-13700.0
1884.8
2214.7
64393.0
-9.6
-10.2
5825.0
26.1
54525.4
19.0
185852.6
5278.0
7270.8
2659.0

A3C FF
518.4
263.9
5474.9
22140.5
4474.5
911091.0
970.1
12950.0
22707.9
817.9
35.1
59.8
681.9
3755.8
7021.0
112646.0
56533.0
113308.4
-0.1
-82.5
18.8
0.1
190.5
10022.8
303.5
32464.1
-2.8
541.0
94.0
5560.0
28819.0
67.0
653.7
10476.1
52894.1
-78.5
5.6
206.9
15148.8
12201.8
34216.0
32.8
2355.4
-10911.1
1956.0
15730.5
138218.0
-9.7
-6.3
12679.0
156.3
74705.7
23.0
331628.1
17244.0
7157.5
24622.0

A3C LSTM
945.3
173.0
14497.9
17244.5
5093.1
875822.0
932.8
20760.0
24622.2
862.2
41.8
37.3
766.8
1997.0
10150.0
138518.0
233021.5
115201.9
0.1
-82.5
22.6
0.1
197.6
17106.8
320.0
28889.5
-1.7
613.0
125.0
5911.4
40835.0
41.0
850.7
12093.7
74786.7
-135.7
10.7
421.1
21307.5
6591.9
73949.0
2.6
1326.1
-14863.8
1936.4
23846.0
164766.0
-8.3
-6.4
27202.0
144.2
105728.7
25.0
470310.5
18082.0
5615.5
23519.0

Table S3. Raw scores for the human start condition (30 minutes emulator time). DQN scores taken from (Nair et al.,
2015). Double DQN scores taken from (Van Hasselt et al., 2015), Dueling scores from (Wang et al., 2015) and Prioritized
scores taken from (Schaul et al., 2015)

