<summary>  We propose a stochastic variance reduced optimization algorithm for solving
sparse learning problems with cardinality constraints. Sufficient conditions
are provided, under which the proposed algorithm enjoys strong linear
convergence guarantees and optimal estimation accuracy in high dimensions. We
further extend the proposed algorithm to an asynchronous parallel variant with
a near linear speedup. Numerical experiments demonstrate the efficiency of our
algorithm in terms of both parameter estimation and computational performance.
</summary><summary>  We study directed, weighted graphs $G=(V,E)$ and consider the (not
necessarily symmetric) averaging operator $$ (\mathcal{L}u)(i) = -\sum_{j
\sim_{} i}{p_{ij} (u(j) - u(i))},$$ where $p_{ij}$ are normalized edge weights.
Given a vertex $i \in V$, we define the diffusion distance to a set $B \subset
V$ as the smallest number of steps $d_{B}(i) \in \mathbb{N}$ required for half
of all random walks started in $i$ and moving randomly with respect to the
weights $p_{ij}$ to visit $B$ within $d_{B}(i)$ steps. Our main result is that
the eigenfunctions interact nicely with this notion of distance. In particular,
if $u$ satisfies $\mathcal{L}u = \lambda u$ on $V$ and $$ B = \left\{ i \in V:
- \varepsilon \leq u(i) \leq \varepsilon \right\} \neq \emptyset,$$ then, for
all $i \in V$, $$ d_{B}(i) \log{\left( \frac{1}{|1-\lambda|} \right) } \geq
\log{\left( \frac{ |u(i)| }{\|u\|_{L^{\infty}}} \right)} -
\log{\left(\frac{1}{2} + \varepsilon\right)}.$$ $d_B(i)$ is a remarkably good
approximation of $|u|$ in the sense of having very high correlation. The result
implies that the classical one-dimensional spectral embedding preserves
particular aspects of geometry in the presence of clustered data. We also give
a continuous variant of the result which has a connection to the hot spots
conjecture.
</summary><summary>  Neural node embeddings have recently emerged as a powerful representation for
supervised learning tasks involving graph-structured data. We leverage this
recent advance to develop a novel algorithm for unsupervised community
discovery in graphs. Through extensive experimental studies on simulated and
real-world data, we demonstrate that the proposed approach consistently
improves over the current state-of-the-art. Specifically, our approach
empirically attains the information-theoretic limits for community recovery
under the benchmark Stochastic Block Models for graph generation and exhibits
better stability and accuracy over both Spectral Clustering and Acyclic Belief
Propagation in the community recovery limits.
</summary><summary>  Generative adversarial networks (GANs) are successful deep generative models.
GANs are based on a two-player minimax game. However, the objective function
derived in the original motivation is changed to obtain stronger gradients when
learning the generator. We propose a novel algorithm that repeats the density
ratio estimation and f-divergence minimization. Our algorithm offers a new
perspective toward the understanding of GANs and is able to make use of
multiple viewpoints obtained in the research of density ratio estimation, e.g.
what divergence is stable and relative density ratio is useful.
</summary><summary>  Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most
popular algorithms for computational inference in Graphical Models (GM). In
principle, MCMC is an exact probabilistic method which, however, often suffers
from exponentially slow mixing. In contrast, BP is a deterministic method,
which is typically fast, empirically very successful, however in general
lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC
algorithms correcting the approximation error of BP, i.e., we provide a way to
compensate for BP errors via a consecutive BP-aware MCMC. Our framework is
based on the Loop Calculus (LC) approach which allows to express the BP error
as a sum of weighted generalized loops. Although the full series is
computationally intractable, it is known that a truncated series, summing up
all 2-regular loops, is computable in polynomial-time for planar pair-wise
binary GMs and it also provides a highly accurate approximation empirically.
Motivated by this, we first propose a polynomial-time approximation MCMC scheme
for the truncated series of general (non-planar) pair-wise binary models. Our
main idea here is to use the Worm algorithm, known to provide fast mixing in
other (related) problems, and then design an appropriate rejection scheme to
sample 2-regular loops. Furthermore, we also design an efficient rejection-free
MCMC scheme for approximating the full series. The main novelty underlying our
design is in utilizing the concept of cycle basis, which provides an efficient
decomposition of the generalized loops. In essence, the proposed MCMC schemes
run on transformed GM built upon the non-trivial BP solution, and our
experiments show that this synthesis of BP and MCMC outperforms both direct
MCMC and bare BP schemes.
</summary>